Spark is used for multi-processing of big data problems. Spark is working on concept of RDD (Resilient Distributed dataset).
RDD is used for loading the data in distributed way on which Transformations and Actions can be performed. Following are present in this code base

* Transformation
    * Filter - filtering of data set
    * Map - Getting transformed output from input set
    * FlatMap - Getting multiple output from one input
    * Union & Intersection - set operations